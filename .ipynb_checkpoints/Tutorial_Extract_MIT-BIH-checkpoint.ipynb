{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9498b722-2ab4-4dc2-8592-ef02e5c4ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import wfdb\n",
    "import copy as cp\n",
    "import scipy.signal as signal\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ee101c-e858-4858-9dc6-1ee1f0a1b052",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract information from MIT-BIH raw Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e6193-ff4a-4c43-b309-4a1e13adfa9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Record Names from the RECORDS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "515825ef-c830-49a3-b6a8-e06c052a528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlist = []\n",
    "records = 'afib-svsm/mit-bih-raw/RECORDS' # Replace the _______ with the name of the records file in mit-bih-raw/\n",
    "with open(records) as rfile: #Then we open the file \n",
    "                             #The 'with' command only opens the file while we are in it. Automatically closes the file when we're not\n",
    "    for record in rfile:  # Then we iterate through the lines in the file\n",
    "        record = record[0:len(record)-1] # Remove any erronious new line characters at the end ('\\n')\n",
    "        rlist.append(record) # Then build an array with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015efad7-23a3-465e-8ff9-62260fe6c3a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract all info using the WaveForm DataBase (WFDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d79390b-897d-4520-8b59-c99f6547d470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampto must be greater than sampfrom\n",
      "sampto must be greater than sampfrom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:02<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### Step 1: Initialize all Arrays\n",
    "             # Below, replace all of the ___ with the command that declares an array/list\n",
    "             # hint: https://stackoverflow.com/questions/1514553/how-to-declare-an-array-in-python\n",
    "samples = [] # will house the samples of all subjects\n",
    "good_list = [] # will list the names of the subjects we successfully extracted\n",
    "bad_list = [] # will house the names of the subjects we failed to extract\n",
    "qrs = [] # will house the indices of R-Peaks for all subjects\n",
    "atr_label = [] # will house the labels for each rhythm annotation for all subjects\n",
    "atr_locs = [] # will house the locations corresponding to the rhythm annotation labels\n",
    "\n",
    "\n",
    "###### Step 2: Extract Information\n",
    "for x in tqdm(rlist): #this will iterate through te records that we found above\n",
    "    try: # A try statement will run the except statement if for some reason the try commands fail\n",
    "         # In this case I use the try statement because one of the subjects has no signal data causing failure\n",
    "         # I then use bad_list and good_list so that all of the indices in rlist match with the arrays we initialized in Step 1, above\n",
    "        ######################################################\n",
    "            # Below find the wfdb function that will return the information that is described below \n",
    "            # Then replace _____ with the correct function call\n",
    "        samp = wfdb.rdsamp('mit-bih-raw/'+x) # wfdb._____(file_location) will read the signal & header data and return a 2 value array\n",
    "            # samp[0] - the signal data is the raw reading from the ecg. Each value is a sample taken.\n",
    "            # samp[1] - the header data includes things about the signal data such as:\n",
    "              # samples per section, denoted 'fs'\n",
    "              # number of signals, denoted 'n_sig'\n",
    "            \n",
    "        ######################################################\n",
    "        samples.append(samp) #add it to our array for all subject\n",
    "        \n",
    "            #What is our file extension that has the annotation we want? Find it here and replace _____ with it \n",
    "            #hint: READ THE VARIABLE NAMES!!!!\n",
    "        qrs_tmp = wfdb.rdann('mit-bih-raw/'+x, extension='qrs') #extract the QRS Info\n",
    "        qrs_locs = np.array(qrs_tmp.sample, dtype='int') #Get just the loccation of R-Peaks from the QRS Info\n",
    "        qrs.append(qrs_locs) # Add to our array for all subjects\n",
    "        \n",
    "        \n",
    "            #Do the same thing here\n",
    "        atr = wfdb.rdann('mit-bih-raw/'+x,extension='atr') #extract the atr info which stores the rhythm type(s) over the whole signal\n",
    "        atr_label.append(atr.aux_note) # aux_note stores the type of rhythm - main two are '(N' for normal and '(AFIB' for AFIB\n",
    "        atr_locs.append(np.append(atr.sample, len(samp[0]))) #I add the length of the whole sample to the end for better visualization later\n",
    "        \n",
    "        good_list.append(x) # when all extraction is successful append the record name to good_list\n",
    "    except Exception as exep:\n",
    "        print(exep) # Alert the user of an exception\n",
    "        bad_list.append(x) # add to the bad list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8203fc16-a7cc-4f58-8977-a064b4abbef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.275, -0.37 ],\n",
      "       [-0.245, -0.45 ],\n",
      "       [-0.305, -0.43 ],\n",
      "       ...,\n",
      "       [-0.24 , -0.52 ],\n",
      "       [-0.3  , -0.52 ],\n",
      "       [-0.27 , -0.565]]), {'fs': 250, 'sig_len': 9205760, 'n_sig': 2, 'base_date': None, 'base_time': None, 'units': ['mV', 'mV'], 'sig_name': ['ECG1', 'ECG2'], 'comments': []})\n"
     ]
    }
   ],
   "source": [
    "print(samp)\n",
    "\n",
    "# Now, in this code block use wfdb to extract the sample info, QRS info, and atr info\n",
    "# Print some stuff out and see if you can figure out how to manipulate it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc226665-5423-4dab-97c7-cda97d9e8781",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extracting Rhythm Data\n",
    "\n",
    "Next, I am going to reformat the rhythm annotations into a different format that is more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb6efa-ee28-435c-be71-87e9b3e7c3a1",
   "metadata": {},
   "source": [
    "The current format for an individuals rhythm annotations are as follows:\n",
    "- `labels = ['(N', '(AFIB', '(N', '(O', ...]`\n",
    "- `locs   = [  10,    1000, 1234, 1983, ...]`\n",
    "\n",
    "Where the labels' corresponding locations are where that rhythm begins.\n",
    "\n",
    "The below code changes it to the following format instead using the python data type Dictionary.\n",
    "\n",
    "```python\n",
    "rhythm_annotations = {\n",
    "    '(N':    [ [10,   999],\n",
    "               [1234, 1982]\n",
    "             ], \n",
    "    '(AFIB': [ [1000, 1233]\n",
    "             ],\n",
    "    '(O':    [ [1983, ...]\n",
    "             ]    \n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "This data is now formatted in a 2-Dimensional array in which each pair of values represents a range of values in which a specific rythm is present. \n",
    "\n",
    "The data can be accessed like so: \n",
    "```python\n",
    "  rhythm_annotations['(N']         = [ [10,   999],\n",
    "                                       [1234, 1982]\n",
    "                                     ]\n",
    "    \n",
    "  rhythm_annotations['(N'][0]      = [10,   999]\n",
    "\n",
    "  rhythm_annotations['(N'][0][0]   = 10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f1c5e59-6ec5-40cb-b6ac-f40a4b70a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "atr_dics = [] #Initialize the array that will hold the dictionary for each subject\n",
    "\n",
    "\n",
    "for idxs,lab in enumerate(atr_label):\n",
    "    atr_dic = {} #Initialize dictionary for each subject\n",
    "    for idx,x in enumerate(lab):\n",
    "        if x not in atr_dic.keys():\n",
    "            atr_dic[x] = [] #Add dictionary key if does not exist\n",
    "        atr_dic[x].append([atr_locs[idxs][idx], atr_locs[idxs][idx+1]]) #Insert range for each rhythm\n",
    "    atr_dics.append(atr_dic) #Add to dictionary array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876588b9-6e6f-4d46-86c5-5dac12203a82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Ways To Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1f7b5-0915-4b47-8532-64ce5bf76c00",
   "metadata": {},
   "source": [
    "Here we are going to format each subjects data into a DataFrame using pandas. Many statistical tools are built to be used with DataFrames. \n",
    "\n",
    "Also, it allows for a one-stop shop for our data where we can save all data for each subject in one file instead of having multiple files per subject.\n",
    "\n",
    "Our data frame is going to be formatted like so:\n",
    "\n",
    "|     | Signal 1 | Signal 2 | R-Peak | Normal | AFIB  | Other |\n",
    "|-----|----------|----------|--------|--------|-------|-------|\n",
    "| ... | ...      | ...      | ...    | ...    | ...   | ...   |\n",
    "| 234 | 0.123    | -0.312   | True   | True   | False | False |\n",
    "| ... | ...      | ...      | ...    | ...    | ...   | ...   |\n",
    "\n",
    "\n",
    "- Column 1: Index\n",
    "    - the index is the value of each row and represents the sample value\n",
    "- Column 2: Signal 1\n",
    "    - a float (or decimal) value which represents the value of the first signal in the reading at the given sample value\n",
    "- Column 3: Signal 2\n",
    "    - a float (or decimal) value which represents the value of the second signal in the reading at the given sample value\n",
    "- Column 4: R-Peak\n",
    "    - a boolean value (```True``` or ```False```) which represents if there is a R-Peak at the given sample value\n",
    "- Column 5: Normal\n",
    "    - a boolean value (```True``` or ```False```) which represents if the sample is in a pattern of Normal beats\n",
    "- Column 6: AFIB\n",
    "    - a boolean value (```True``` or ```False```) which represents if the sample is in a pattern of AFIB beats\n",
    "- Column 7: Other\n",
    "    - a boolean value (```True``` or ```False```) which represents if the sample is in a pattern of other beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "762e9247-602d-46f8-adee-d9eca9c54456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [09:40<00:00, 25.25s/it]\n"
     ]
    }
   ],
   "source": [
    "subject_dataframes = [] # Initialize the subject_dataframes - will hold all of our subject dataframes\n",
    "\n",
    "for s, _ in enumerate(tqdm(good_list)): # Iterate through all of the subjects that we have complete data of \n",
    "    subj = pd.DataFrame( # The below statements initialize our datafram. The first to columns will be our given signals, and the rest we initialize to 0\n",
    "        data = np.transpose(np.array([ # First we give our data, for pandas they want the data by row instead of by column, so we use transpose to get the proper format\n",
    "                                               [x[0] for x in samples[s][0]],\n",
    "                                               [x[1] for x in samples[s][0]],\n",
    "                                               np.zeros(len(samples[s][0])), # np.zeros makes an array of zeros with the given lenth\n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                               np.zeros(len(samples[s][0])), \n",
    "                                        ])\n",
    "                           ),\n",
    "        columns = ['Signal 1', 'Signal 2', 'R-Peak', 'Normal', 'AFIB', 'Other'] # Here we name our columns to match the dataframe we outlined above\n",
    "    )\n",
    "    norm = [] # Initialize the norm array which will list every index the person is in a normal rhythm\n",
    "    if '(N' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(N']: # Then we iterate through our ranges we extracted above\n",
    "            norm = norm + list(range(x[0], x[1])) # And add all values in the range to our norm array\n",
    "    af = [] # Then we do the same steps above for AFIB rhythms\n",
    "    if '(AFIB' in atr_dics[s].keys():\n",
    "        for x in atr_dics[s]['(AFIB']:\n",
    "            af = af + list(range(x[0], x[1]))\n",
    "    subj['R-Peak']= subj.index.isin(qrs[s]) # the isin() function of a DataFram index will return true if the index is in that list and false if it is not\n",
    "                                            # then, we can initialize our dataFrame with correct values based on that\n",
    "    subj['Normal']= subj.index.isin(norm)\n",
    "    subj['AFIB'] = subj.index.isin(af)\n",
    "    subj['Other'] = ~subj.index.isin(np.append(norm, af)) # Because we are classifying AFIB specifically we define other as any rhythm not in the norm or AFIB list\n",
    "    \n",
    "    subject_dataframes.append(subj) # Add the dataframe we built to our to array that holds all of our subjects' dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4366c10-1a4b-49d7-88b4-de98ac4ba850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Signal 1</th>\n",
       "      <th>Signal 2</th>\n",
       "      <th>R-Peak</th>\n",
       "      <th>Normal</th>\n",
       "      <th>AFIB</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.415</td>\n",
       "      <td>-0.260</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.430</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.460</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205755</th>\n",
       "      <td>0.220</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205756</th>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205757</th>\n",
       "      <td>0.175</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205758</th>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9205759</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9205760 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Signal 1  Signal 2  R-Peak  Normal   AFIB  Other\n",
       "0          -0.415    -0.395   False   False  False   True\n",
       "1          -0.415    -0.260   False   False  False   True\n",
       "2          -0.430    -0.185   False   False  False   True\n",
       "3          -0.445    -0.135   False   False  False   True\n",
       "4          -0.460    -0.080   False   False  False   True\n",
       "...           ...       ...     ...     ...    ...    ...\n",
       "9205755     0.220    -0.130   False    True  False  False\n",
       "9205756     0.200    -0.160   False    True  False  False\n",
       "9205757     0.175    -0.140   False    True  False  False\n",
       "9205758     0.080    -0.160   False    True  False  False\n",
       "9205759    -0.030    -0.250   False    True  False  False\n",
       "\n",
       "[9205760 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_dataframes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df88299a-2a33-4429-a5bc-faf7653ec2c5",
   "metadata": {},
   "source": [
    "## Saving Extracted Information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9c4d4-f4de-4a11-aa67-a82f9b2f4d13",
   "metadata": {},
   "source": [
    "Saving the information that we have used processing power to extract is important because:\n",
    "1. It makes our data easier to access in the future\n",
    "    - Easy access in new files\n",
    "2. It creates static information for us to use and reference\n",
    "3. By saving in in a CSV we make it more accessible for others to use\n",
    "    - The data can now be used in an excel sheet and more\n",
    "    \n",
    "We will generally always extract to a CSV file unless the data is too complex. If that is the case then we have another option. \n",
    "\n",
    "'pickle' is a Python package which will save much more complex Data types for future use. \n",
    "\n",
    "For example - if you have want to save a statistical model, pickle will be able to do that more effectively than CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dd24094-b73a-43a1-ab35-212654d9c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_flag = False\n",
    "  # Set this flag to true to re-save all of the extracted information even if it has already been saved at these paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c931201-1ff5-4ef2-bf6c-203e18c754af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 19/23 [27:56<09:23, 140.86s/it]"
     ]
    }
   ],
   "source": [
    "for idx, x in enumerate(tqdm(good_list)): \n",
    "    if not os.path.exists('mit-bih-dataframes/'+x+ '.csv') or reload_flag:\n",
    "        subject_dataframes[idx].to_csv('afib-svsm/mit-bih-dataframes/'+x+'.csv') # Pandas DataFrames have a built in to_csv() function which whill save it at the passed path\n",
    "\n",
    "np.savetxt(\"mit-bih-dataframes/subject_list.csv\", good_list, delimiter=\",\",  fmt='%s') \n",
    "   # We'll load the complete list of subjects as well so that we can easily recreate the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f60d8b-d0ca-4736-b898-f00835bfe431",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"mit-bih-extracted/subject_list.csv\", good_list, delimiter=\",\",  fmt='%s') #Save the names in the folder \n",
    "for idx, x in enumerate(tqdm(good_list)): # Iterate through our subjects\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_signals.csv\") or reload_flag:\n",
    "        np.savetxt(\"mit-bih-extracted/\"+x+\"_signals.csv\", np.array(samples[idx][0]), delimiter=\",\") # numPy has a savetxt() function which by setting the delimiter as ',' we can \n",
    "                                                                                            # simulate a to_csv() function \n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rpeaks.csv\") or reload_flag:\n",
    "            np.savetxt(\"mit-bih-extracted/\"+x+\"_rpeaks.csv\", np.array(qrs[idx]), delimiter=\",\")      \n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_headers.pkl\") or reload_flag:\n",
    "        with open(\"mit-bih-extracted/\"+x+\"_headers.pkl\", 'wb') as picklefile: # nomPy has no way to save a dictionary as a CSV so we use the pickle package\n",
    "                                    # First we open up the file we would like to write to\n",
    "            pickle.dump(samples[idx][1], picklefile)\n",
    "    if not os.path.exists(\"mit-bih-extracted/\"+x+\"_rhythms.pkl\") or reload_flag:\n",
    "        with open(\"mit-bih-extracted/\"+x+\"_rhythms.pkl\", 'wb') as picklefile:\n",
    "            pickle.dump(atr_dics[idx], picklefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12ad29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
